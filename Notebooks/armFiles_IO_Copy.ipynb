{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lourenço Cavalcante\\AppData\\Local\\Temp\\ipykernel_11080\\2966471990.py:509: AccessorRegistrationWarning: registration of accessor <class '__main__.WriteDataset'> under name 'write' for type <class 'xarray.core.dataset.Dataset'> is overriding a preexisting attribute with the same name.\n",
      "  @xr.register_dataset_accessor('write')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This module contains I/O operations for loading files that were created for the\n",
    "Atmospheric Radiation Measurement program supported by the Department of Energy\n",
    "Office of Science.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Este módulo contém operações de E/S para carregar arquivos que foram criados para o\n",
    "programa de Medição de Radiação Atmosférica apoiado pelo Departamento de Energia\n",
    "do Office of Science.\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path, PosixPath\n",
    "from netCDF4 import Dataset\n",
    "from os import PathLike\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import act.utils as utils\n",
    "from act.config import DEFAULT_DATASTREAM_NAME\n",
    "from act.utils.io_utils import unpack_tar, unpack_gzip, cleanup_files, is_gunzip_file\n",
    "\n",
    "\n",
    "def read_netcdf(\n",
    "    filenames,\n",
    "    concat_dim=None,\n",
    "    return_None=False,\n",
    "    combine='by_coords',\n",
    "    decode_times=True,\n",
    "    use_cftime=True,\n",
    "    use_base_time=False,\n",
    "    combine_attrs='override',\n",
    "    cleanup_qc=False,\n",
    "    keep_variables=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns `xarray.Dataset` with stored data and metadata from a user-defined\n",
    "    query of ARM-standard netCDF files from a single datastream. Has some procedures\n",
    "    to ensure time is correctly fomatted in returned Dataset.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filenames : str, pathlib.PosixPath, list of str, list of pathlib.PosixPath\n",
    "        Name of file(s) to read.\n",
    "    concat_dim : str\n",
    "        Dimension to concatenate files along.\n",
    "    return_None : boolean\n",
    "        Catch IOError exception when file not found and return None.\n",
    "        Default is False.\n",
    "    combine : str\n",
    "        String used by xarray.open_mfdataset() to determine how to combine\n",
    "        data files into one Dataset. See Xarray documentation for options.\n",
    "    decode_times : boolean\n",
    "        Standard Xarray option to decode time values from int/float to python datetime values.\n",
    "        Appears the default is to do this anyway but need this option to allow correct usage\n",
    "        of use_base_time.\n",
    "    use_cftime : boolean\n",
    "        Option to use cftime library to parse the time units string and correctly\n",
    "        establish the time values with a units string containing timezone offset.\n",
    "        This is used because the Pandas units string parser does not correctly recognize\n",
    "        time zone offset. Code will automatically detect cftime object and convert to datetime64\n",
    "        in returned Dataset.\n",
    "    use_base_time : boolean\n",
    "        Option to use ARM time variables base_time and time_offset. Useful when the time variable\n",
    "        is not included (older files) or when the units attribute is incorrectly formatted. Will use\n",
    "        the values of base_time and time_offset as seconds since epoch and create datetime64 values\n",
    "        for time coordinate. If set will change decode_times and use_cftime to False.\n",
    "    combine_attrs : str\n",
    "        String indicating how to combine attrs of the objects being merged\n",
    "    cleanup_qc : boolean\n",
    "        Call clean.cleanup() method to convert to standardized ancillary quality control\n",
    "        variables. This will not allow any keyword options, so if non-default behavior is\n",
    "        desired will need to call clean.cleanup() method on the object after reading the data.\n",
    "    keep_variables : str or list of str\n",
    "        Variable names to read from data file. Works by creating a list of variable names\n",
    "        to exclude from reading and passing into open_mfdataset() via drop_variables keyword.\n",
    "        Still allows use of drop_variables keyword for variables not listed in first file to\n",
    "        read.\n",
    "    **kwargs : keywords\n",
    "        Keywords to pass through to xarray.open_mfdataset().\n",
    "    Returns\n",
    "    -------\n",
    "    obj : Object (or None)\n",
    "        ACT dataset (or None if no data file(s) found).\n",
    "    Examples\n",
    "    --------\n",
    "    This example will load the example sounding data used for unit testing.\n",
    "    .. code-block :: python\n",
    "        import act\n",
    "        ds = act.io.armfiles.read_netcdf(act.tests.sample_files.EXAMPLE_SONDE_WILDCARD)\n",
    "        print(ds)\n",
    "    \"\"\"\n",
    "\n",
    "    ds = None\n",
    "    filenames, cleanup_temp_directory = check_if_tar_gz_file(filenames)\n",
    "\n",
    "    file_dates = []\n",
    "    file_times = []\n",
    "\n",
    "    # If requested to use base_time and time_offset, set keywords to correct attribute values\n",
    "    # to pass into xarray open_mfdataset(). Need to turn off decode_times and use_cftime\n",
    "    # or else will try to convert base_time and time_offset. Depending on values of attributes\n",
    "    # may cause a failure.\n",
    "    if use_base_time:\n",
    "        decode_times = False\n",
    "        use_cftime = False\n",
    "\n",
    "    # Add funciton keywords to kwargs dictionary for passing into open_mfdataset.\n",
    "    kwargs['combine'] = combine\n",
    "    kwargs['concat_dim'] = concat_dim\n",
    "    kwargs['decode_times'] = decode_times\n",
    "    kwargs['use_cftime'] = use_cftime\n",
    "    if len(filenames) > 1 and not isinstance(filenames, str):\n",
    "        kwargs['combine_attrs'] = combine_attrs\n",
    "\n",
    "    # Check if keep_variables is set. If so determine correct drop_variables\n",
    "    if keep_variables is not None:\n",
    "        drop_variables = None\n",
    "        if 'drop_variables' in kwargs.keys():\n",
    "            drop_variables = kwargs['drop_variables']\n",
    "        kwargs['drop_variables'] = keep_variables_to_drop_variables(\n",
    "            filenames, keep_variables, drop_variables=drop_variables)\n",
    "\n",
    "    # Create an exception tuple to use with try statements. Doing it this way\n",
    "    # so we can add the FileNotFoundError if requested. Can add more error\n",
    "    # handling in the future.\n",
    "    except_tuple = (ValueError,)\n",
    "    if return_None:\n",
    "        except_tuple = except_tuple + (FileNotFoundError, OSError)\n",
    "\n",
    "    try:\n",
    "        # Read data file with Xarray function\n",
    "        ds = xr.open_mfdataset(filenames, **kwargs)\n",
    "\n",
    "    except except_tuple as exception:\n",
    "        # If requested return None for File not found error\n",
    "        if type(exception).__name__ == 'FileNotFoundError':\n",
    "            return None\n",
    "\n",
    "        # If requested return None for File not found error\n",
    "        if type(exception).__name__ == 'OSError' and exception.args[0] == 'no files to open':\n",
    "            return None\n",
    "\n",
    "        # Look at error message and see if could be nested error message. If so\n",
    "        # update combine keyword and try again. This should allow reading files\n",
    "        # without a time variable but base_time and time_offset variables.\n",
    "        if (\n",
    "            kwargs['combine'] != 'nested'\n",
    "            and type(exception).__name__ == 'ValueError'\n",
    "            and exception.args[0] == 'Could not find any dimension coordinates '\n",
    "            'to use to order the datasets for concatenation'\n",
    "        ):\n",
    "            kwargs['combine'] = 'nested'\n",
    "            ds = xr.open_mfdataset(filenames, **kwargs)\n",
    "\n",
    "        else:\n",
    "            # When all else fails raise the orginal exception\n",
    "            raise exception\n",
    "\n",
    "    # If requested use base_time and time_offset to derive time. Assumes that the units\n",
    "    # of both are in seconds and that the value is number of seconds since epoch.\n",
    "    if use_base_time:\n",
    "        time = (ds['base_time'].values + ds['time_offset'].values) * 1000000.0\n",
    "        time = np.array(time, dtype='datetime64[us]')\n",
    "\n",
    "        # Need to use a new Dataset creation to correctly index time for use with\n",
    "        # .group and .resample methods in Xarray Datasets.\n",
    "        temp_ds = xr.Dataset({'time': (ds['time'].dims, time, ds['time'].attrs)})\n",
    "        ds['time'] = temp_ds['time']\n",
    "        del temp_ds\n",
    "        for att_name in ['units', 'ancillary_variables']:\n",
    "            try:\n",
    "                del ds['time'].attrs[att_name]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    # Xarray has issues reading a CF formatted time units string if it contains\n",
    "    # timezone offset without a [+|-] preceeding timezone offset.\n",
    "    # https://github.com/pydata/xarray/issues/3644\n",
    "    # To ensure the times are read in correctly need to set use_cftime=True.\n",
    "    # This will read in time as cftime object. But Xarray uses numpy datetime64\n",
    "    # natively. This will convert the cftime time values to numpy datetime64. cftime\n",
    "    # does not preserve the time past ms precision. We will use ms precision for\n",
    "    # the conversion.\n",
    "    desired_time_precision = 'datetime64[ms]'\n",
    "    for var_name in ['time', 'time_offset']:\n",
    "        try:\n",
    "            if 'time' in ds.dims and type(ds[var_name].values[0]).__module__.startswith('cftime.'):\n",
    "                # If we just convert time to datetime64 the group, sel, and other Xarray\n",
    "                # methods will not work correctly because time is not indexed. Need to\n",
    "                # use the formation of a Dataset to correctly set the time indexing.\n",
    "                temp_ds = xr.Dataset(\n",
    "                    {\n",
    "                        var_name: (\n",
    "                            ds[var_name].dims,\n",
    "                            ds[var_name].values.astype(desired_time_precision),\n",
    "                            ds[var_name].attrs,\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "                ds[var_name] = temp_ds[var_name]\n",
    "                del temp_ds\n",
    "\n",
    "                # If time_offset is in file try to convert base_time as well\n",
    "                if var_name == 'time_offset':\n",
    "                    ds['base_time'].values = ds['base_time'].values.astype(desired_time_precision)\n",
    "                    ds['base_time'] = ds['base_time'].astype(desired_time_precision)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # Check if \"time\" variable is not in the netCDF file. If so try to use\n",
    "    # base_time and time_offset to make time variable. Basically a fix for incorrectly\n",
    "    # formatted files. May require using decode_times=False to initially read the data.\n",
    "    if 'time' in ds.dims and not np.issubdtype(ds['time'].dtype, np.datetime64):\n",
    "        try:\n",
    "            ds['time'] = ds['time_offset']\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "\n",
    "    # Adding support for wildcards\n",
    "    if isinstance(filenames, str):\n",
    "        filenames = glob.glob(filenames)\n",
    "    elif isinstance(filenames, PosixPath):\n",
    "        filenames = [filenames]\n",
    "\n",
    "    # Get file dates and times that were read in to the object\n",
    "    filenames.sort()\n",
    "    for f in filenames:\n",
    "        f = Path(f).name\n",
    "        pts = re.match(r'(^[a-zA-Z0-9]+)\\.([0-9a-z]{2})\\.([\\d]{8})\\.([\\d]{6})\\.([a-z]{2,3}$)', f)\n",
    "        # If Not ARM format, read in first time for info\n",
    "        if pts is not None:\n",
    "            pts = pts.groups()\n",
    "            file_dates.append(pts[2])\n",
    "            file_times.append(pts[3])\n",
    "        else:\n",
    "            if ds['time'].size > 1:\n",
    "                dummy = ds['time'].values[0]\n",
    "            else:\n",
    "                dummy = ds['time'].values\n",
    "            file_dates.append(utils.numpy_to_arm_date(dummy))\n",
    "            file_times.append(utils.numpy_to_arm_date(dummy, returnTime=True))\n",
    "\n",
    "    # Add attributes\n",
    "    ds.attrs['_file_dates'] = file_dates\n",
    "    ds.attrs['_file_times'] = file_times\n",
    "    is_arm_file_flag = check_arm_standards(ds)\n",
    "\n",
    "    # Ensure that we have _datastream set whether or no there's\n",
    "    # a datastream attribute already.\n",
    "    if is_arm_file_flag == 0:\n",
    "        ds.attrs['_datastream'] = DEFAULT_DATASTREAM_NAME\n",
    "    else:\n",
    "        ds.attrs['_datastream'] = ds.attrs['datastream']\n",
    "\n",
    "    ds.attrs['_arm_standards_flag'] = is_arm_file_flag\n",
    "\n",
    "    if cleanup_qc:\n",
    "        ds.clean.cleanup()\n",
    "\n",
    "    if cleanup_temp_directory:\n",
    "        cleanup_files(files=filenames)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def keep_variables_to_drop_variables(\n",
    "        filenames,\n",
    "        keep_variables,\n",
    "        drop_variables=None):\n",
    "    \"\"\"\n",
    "    Returns a list of variable names to exclude from reading by passing into\n",
    "    `Xarray.open_dataset` drop_variables keyword. This can greatly help reduce\n",
    "    loading time and disk space use of the Dataset.\n",
    "    When passed a netCDF file name, will open the file using the netCDF4 library to get\n",
    "    list of variable names. There is less overhead reading the varible names using\n",
    "    netCDF4 library than Xarray. If more than one filename is provided or string is\n",
    "    used for shell syntax globbing, will use the first file in the list.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filenames : str, pathlib.PosixPath or list of str\n",
    "        Name of file(s) to read.\n",
    "    keep_variables : str or list of str\n",
    "        Variable names desired to keep. Do not need to list associated dimention\n",
    "        names. These will be automatically kept as well.\n",
    "    drop_variables : str or list of str\n",
    "        Variable names to explicitly add to returned list. May be helpful if a variable\n",
    "        exists in a file that is not in the first file in the list.\n",
    "    Returns\n",
    "    -------\n",
    "    obj : list of str\n",
    "        Variable names to exclude from returned Dataset by using drop_variables keyword\n",
    "        when calling Xarray.open_dataset().\n",
    "    Examples\n",
    "    --------\n",
    "    .. code-block :: python\n",
    "        import act\n",
    "        filename = '/data/datastream/hou/houkasacrcfrM1.a1/houkasacrcfrM1.a1.20220404.*.nc'\n",
    "        drop_vars = act.io.armfiles.keep_variables_to_drop_variables(\n",
    "            filename, ['lat','lon','alt','crosspolar_differential_phase'],\n",
    "            drop_variables='variable_name_that_only_exists_in_last_file_of_the_day')\n",
    "    \"\"\"\n",
    "    read_variables = []\n",
    "    return_variables = []\n",
    "\n",
    "    if isinstance(keep_variables, str):\n",
    "        keep_variables = [keep_variables]\n",
    "\n",
    "    if isinstance(drop_variables, str):\n",
    "        drop_variables = [drop_variables]\n",
    "\n",
    "    # If filenames is a list subset to first file name.\n",
    "    if isinstance(filenames, (list, tuple)):\n",
    "        filename = filenames[0]\n",
    "    # If filenames is a string, check if it needs to be expanded in shell\n",
    "    # first. Then use first returned file name. Else use the string filename.\n",
    "    elif isinstance(filenames, str):\n",
    "        filename = glob.glob(filenames)\n",
    "        if len(filename) == 0:\n",
    "            return return_variables\n",
    "        else:\n",
    "            filename.sort()\n",
    "            filename = filename[0]\n",
    "\n",
    "    # Use netCDF4 library to extract the variable and dimension names.\n",
    "    rootgrp = Dataset(filename, 'r')\n",
    "    read_variables = list(rootgrp.variables)\n",
    "    dimensions = list(rootgrp.dimensions)\n",
    "    # Loop over the variables to exclude needed coordinate dimention names.\n",
    "    dims_to_keep = []\n",
    "    for var_name in keep_variables:\n",
    "        try:\n",
    "            dims_to_keep.extend(list(rootgrp[var_name].dimensions))\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    rootgrp.close()\n",
    "\n",
    "    # Remove names not matching keep_varibles excluding the associated coordinate dimentions\n",
    "    return_variables = set(read_variables) - set(keep_variables) - set(dims_to_keep)\n",
    "\n",
    "    # Add drop_variables to list\n",
    "    if drop_variables is not None:\n",
    "        return_variables = set(return_variables) | set(drop_variables)\n",
    "\n",
    "    return list(return_variables)\n",
    "\n",
    "\n",
    "def check_arm_standards(ds):\n",
    "    \"\"\"\n",
    "    Checks to see if an xarray dataset conforms to ARM standards.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray dataset\n",
    "        The dataset to check.\n",
    "    Returns\n",
    "    -------\n",
    "    flag : int\n",
    "        The flag corresponding to whether or not the file conforms\n",
    "        to ARM standards. Bit packed, so 0 for no, 1 for yes\n",
    "    \"\"\"\n",
    "    the_flag = 1 << 0\n",
    "    if 'datastream' not in ds.attrs.keys():\n",
    "        the_flag = 0\n",
    "\n",
    "    # Check if the historical global attribute name is\n",
    "    # used instead of updated name of 'datastream'. If so\n",
    "    # correct the global attributes and flip flag.\n",
    "    if 'zeb_platform' in ds.attrs.keys():\n",
    "        ds.attrs['datastream'] = copy.copy(ds.attrs['zeb_platform'])\n",
    "        del ds.attrs['zeb_platform']\n",
    "        the_flag = 1 << 0\n",
    "\n",
    "    return the_flag\n",
    "\n",
    "\n",
    "def create_obj_from_arm_dod(proc, set_dims, version='', fill_value=-9999.0, scalar_fill_dim=None, local_file=False):\n",
    "    \"\"\"\n",
    "    Queries the ARM DOD api and builds an object based on the ARM DOD and\n",
    "    the dimension sizes that are passed in.\n",
    "    Parameters\n",
    "    ----------\n",
    "    proc : string\n",
    "        Process to create the object off of. This is normally in the\n",
    "        format of inst.level. i.e. vdis.b1 or kazrge.a1. If local file\n",
    "        is true, this points to the path of the .dod file.\n",
    "    set_dims : dict\n",
    "        Dictionary of dims from the DOD and the corresponding sizes.\n",
    "        Time is required. Code will try and pull from DOD, unless set\n",
    "        through this variable\n",
    "        Note: names need to match exactly what is in the dod\n",
    "        i.e. {'drop_diameter': 50, 'time': 1440}\n",
    "    version : string\n",
    "        Version number of the ingest to use. If not set, defaults to\n",
    "        latest version\n",
    "    fill_value : float\n",
    "        Fill value for non-dimension variables. Dimensions cannot have\n",
    "        duplicate values and are incrementally set (0, 1, 2)\n",
    "    scalar_fill_dim : str\n",
    "        Depending on how the object is set up, sometimes the scalar values\n",
    "        are dimensioned to the main dimension. i.e. a lat/lon is set to have\n",
    "        a dimension of time. This is a way to set it up similarly.\n",
    "    local_file: bool\n",
    "        If true, the DOD will be loaded from a file whose name is proc.\n",
    "        If false, the DOD will be pulled from PCM.\n",
    "    Returns\n",
    "    -------\n",
    "    obj : xarray Dataset\n",
    "        ACT object populated with all variables and attributes.\n",
    "    Examples\n",
    "    --------\n",
    "    .. code-block :: python\n",
    "        dims = {'time': 1440, 'drop_diameter': 50}\n",
    "        obj = act.io.armfiles.create_obj_from_arm_dod(\n",
    "            'vdis.b1', dims, version='1.2', scalar_fill_dim='time')\n",
    "    \"\"\"\n",
    "    # Set base url to get DOD information\n",
    "    if local_file is False:\n",
    "        base_url = 'https://pcm.arm.gov/pcm/api/dods/'\n",
    "\n",
    "        # Get data from DOD api\n",
    "        with urllib.request.urlopen(base_url + proc) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "    else:\n",
    "        with open(proc) as file:\n",
    "            data = json.loads(file.read())\n",
    "\n",
    "    # Check version numbers and alert if requested version in not available\n",
    "    keys = list(data['versions'].keys())\n",
    "    if version not in keys:\n",
    "        warnings.warn(\n",
    "            ' '.join(\n",
    "                ['Version:', version, 'not available or not specified. Using Version:', keys[-1]]\n",
    "            ),\n",
    "            UserWarning,\n",
    "        )\n",
    "        version = keys[-1]\n",
    "\n",
    "    # Create empty xarray dataset\n",
    "    obj = xr.Dataset()\n",
    "\n",
    "    # Get the global attributes and add to dataset\n",
    "    atts = {}\n",
    "    for a in data['versions'][version]['atts']:\n",
    "        if a['name'] == 'string':\n",
    "            continue\n",
    "        if a['value'] is None:\n",
    "            a['value'] = ''\n",
    "        atts[a['name']] = a['value']\n",
    "\n",
    "    obj.attrs = atts\n",
    "\n",
    "    # Get variable information and create dataarrays that are\n",
    "    # then added to the dataset\n",
    "    # If not passed in through set_dims, will look to the DOD\n",
    "    # if not set in the DOD, then will raise error\n",
    "    variables = data['versions'][version]['vars']\n",
    "    dod_dims = data['versions'][version]['dims']\n",
    "    for d in dod_dims:\n",
    "        if d['name'] not in list(set_dims.keys()):\n",
    "            if d['length'] > 0:\n",
    "                set_dims[d['name']] = d['length']\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'Dimension length not set in DOD for '\n",
    "                    + d['name']\n",
    "                    + ', nor passed in through set_dim'\n",
    "                )\n",
    "    for v in variables:\n",
    "        dims = v['dims']\n",
    "        dim_shape = []\n",
    "        # Using provided dimension data, fill array accordingly for easy overwrite\n",
    "        if len(dims) == 0:\n",
    "            if scalar_fill_dim is None:\n",
    "                data_na = fill_value\n",
    "            else:\n",
    "                data_na = np.full(set_dims[scalar_fill_dim], fill_value)\n",
    "                v['dims'] = scalar_fill_dim\n",
    "        else:\n",
    "            for d in dims:\n",
    "                dim_shape.append(set_dims[d])\n",
    "            if len(dim_shape) == 1 and v['name'] == dims[0]:\n",
    "                data_na = np.arange(dim_shape[0])\n",
    "            else:\n",
    "                data_na = np.full(dim_shape, fill_value)\n",
    "\n",
    "        # Get attribute information. Had to do some things to get to print to netcdf\n",
    "        atts = {}\n",
    "        str_flag = False\n",
    "        for a in v['atts']:\n",
    "            if a['name'] == 'string':\n",
    "                str_flag = True\n",
    "                continue\n",
    "            if a['value'] is None:\n",
    "                continue\n",
    "            if str_flag and a['name'] == 'units':\n",
    "                continue\n",
    "            atts[a['name']] = a['value']\n",
    "\n",
    "        da = xr.DataArray(data=data_na, dims=v['dims'], name=v['name'], attrs=atts)\n",
    "        obj[v['name']] = da\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "@xr.register_dataset_accessor('write')\n",
    "class WriteDataset:\n",
    "    \"\"\"\n",
    "    Class for cleaning up Dataset before writing to file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, xarray_obj):\n",
    "        self._obj = xarray_obj\n",
    "\n",
    "    def write_netcdf(\n",
    "        self,\n",
    "        cleanup_global_atts=True,\n",
    "        cleanup_qc_atts=True,\n",
    "        join_char='__',\n",
    "        make_copy=True,\n",
    "        cf_compliant=False,\n",
    "        delete_global_attrs=['qc_standards_version', 'qc_method', 'qc_comment'],\n",
    "        FillValue=-9999,\n",
    "        cf_convention='CF-1.8',\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This is a wrapper around Dataset.to_netcdf to clean up the Dataset before\n",
    "        writing to disk. Some things are added to global attributes during ACT reading\n",
    "        process, and QC variables attributes are modified during QC cleanup process.\n",
    "        This will modify before writing to disk to better\n",
    "        match Climate & Forecast standards.\n",
    "        Parameters\n",
    "        ----------\n",
    "        cleanup_global_atts : boolean\n",
    "            Option to cleanup global attributes by removing any global attribute\n",
    "            that starts with an underscore.\n",
    "        cleanup_qc_atts : boolean\n",
    "            Option to convert attributes that would be written as string array\n",
    "            to be a single character string. CF 1.7 does not allow string attribures.\n",
    "            Will use a single space a delimeter between values and join_char to replace\n",
    "            white space between words.\n",
    "        join_char : str\n",
    "            The character sting to use for replacing white spaces between words when converting\n",
    "            a list of strings to single character string attributes.\n",
    "        make_copy : boolean\n",
    "            Make a copy before modifying Dataset to write. For large Datasets this\n",
    "            may add processing time and memory. If modifying the Dataset is OK\n",
    "            try setting to False.\n",
    "        cf_compliant : boolean\n",
    "            Option to output file with additional attributes to make file Climate & Forecast\n",
    "            complient. May require runing .clean.cleanup() method on the object to fix other\n",
    "            issues first. This does the best it can but it may not be truely complient. You\n",
    "            should read the CF documents and try to make complient before writing to file.\n",
    "        delete_global_attrs : list\n",
    "            Optional global attributes to be deleted. Defaults to some standard\n",
    "            QC attributes that are not needed. Can add more or set to None to not\n",
    "            remove the attributes.\n",
    "        FillValue : int, float\n",
    "            The value to use as a _FillValue in output file. This is used to fix\n",
    "            issues with how Xarray handles missing_value upon reading. It's confusing\n",
    "            so not a perfect fix. Set to None to leave Xarray to do what it wants.\n",
    "            Set to a value to be the value used as _FillValue in the file and data\n",
    "            array. This should then remove missing_value attribute from the file as well.\n",
    "        cf_convention : str\n",
    "            The Climate and Forecast convention string to add to Conventions attribute.\n",
    "        **kwargs : keywords\n",
    "            Keywords to pass through to Dataset.to_netcdf()\n",
    "        Examples\n",
    "        --------\n",
    "        .. code-block :: python\n",
    "            ds_object.write.write_netcdf(path='output.nc')\n",
    "        \"\"\"\n",
    "\n",
    "        if make_copy:\n",
    "            write_obj = copy.deepcopy(self._obj)\n",
    "        else:\n",
    "            write_obj = self._obj\n",
    "\n",
    "        encoding = {}\n",
    "        if cleanup_global_atts:\n",
    "            for attr in list(write_obj.attrs):\n",
    "                if attr.startswith('_'):\n",
    "                    del write_obj.attrs[attr]\n",
    "\n",
    "        if cleanup_qc_atts:\n",
    "            check_atts = ['flag_meanings', 'flag_assessments']\n",
    "            for var_name in list(write_obj.data_vars):\n",
    "                if 'standard_name' not in write_obj[var_name].attrs.keys():\n",
    "                    continue\n",
    "\n",
    "                for attr_name in check_atts:\n",
    "                    try:\n",
    "                        att_values = write_obj[var_name].attrs[attr_name]\n",
    "                        if isinstance(att_values, (list, tuple)):\n",
    "                            att_values = [att_value.replace(' ', join_char) for att_value in att_values]\n",
    "                            write_obj[var_name].attrs[attr_name] = ' '.join(att_values)\n",
    "\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "\n",
    "                # Tell .to_netcdf() to not add a _FillValue attribute for\n",
    "                # quality control variables.\n",
    "                if FillValue is not None:\n",
    "                    encoding[var_name] = {'_FillValue': None}\n",
    "\n",
    "            # Clean up _FillValue vs missing_value mess by creating an\n",
    "            # encoding dictionary with each variable's _FillValue set to\n",
    "            # requested fill value. May need to improve upon this for data type\n",
    "            # and other issues in the future.\n",
    "            if FillValue is not None:\n",
    "                skip_variables = ['base_time', 'time_offset', 'qc_time'] + list(encoding.keys())\n",
    "                for var_name in list(write_obj.data_vars):\n",
    "                    if var_name not in skip_variables:\n",
    "                        encoding[var_name] = {'_FillValue': FillValue}\n",
    "\n",
    "        if delete_global_attrs is not None:\n",
    "            for attr in delete_global_attrs:\n",
    "                try:\n",
    "                    del write_obj.attrs[attr]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        # If requested update global attributes and variables attributes for required\n",
    "        # CF attributes.\n",
    "        if cf_compliant:\n",
    "            # Get variable names and standard name for each variable\n",
    "            var_names = list(write_obj.keys())\n",
    "            standard_names = []\n",
    "            for var_name in var_names:\n",
    "                try:\n",
    "                    standard_names.append(write_obj[var_name].attrs['standard_name'])\n",
    "                except KeyError:\n",
    "                    standard_names.append(None)\n",
    "\n",
    "            # Check if time varible has axis and standard_name attribute\n",
    "            coord_name = 'time'\n",
    "            try:\n",
    "                write_obj[coord_name].attrs['axis']\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    write_obj[coord_name].attrs['axis'] = 'T'\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "            try:\n",
    "                write_obj[coord_name].attrs['standard_name']\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    write_obj[coord_name].attrs['standard_name'] = 'time'\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "            # Try to determine type of dataset by coordinate dimention named time\n",
    "            # and other factors\n",
    "            try:\n",
    "                write_obj.attrs['FeatureType']\n",
    "            except KeyError:\n",
    "                dim_names = list(write_obj.dims)\n",
    "                FeatureType = None\n",
    "                if dim_names == ['time']:\n",
    "                    FeatureType = 'timeSeries'\n",
    "                elif len(dim_names) == 2 and 'time' in dim_names and 'bound' in dim_names:\n",
    "                    FeatureType = 'timeSeries'\n",
    "                elif len(dim_names) >= 2 and 'time' in dim_names:\n",
    "                    for var_name in var_names:\n",
    "                        dims = list(write_obj[var_name].dims)\n",
    "                        if len(dims) == 2 and 'time' in dims:\n",
    "                            prof_dim = list(set(dims) - {'time'})[0]\n",
    "                            if write_obj[prof_dim].values.size > 2:\n",
    "                                FeatureType = 'timeSeriesProfile'\n",
    "                                break\n",
    "\n",
    "                if FeatureType is not None:\n",
    "                    write_obj.attrs['FeatureType'] = FeatureType\n",
    "\n",
    "            # Add axis and positive attributes to variables with standard_name\n",
    "            # equal to 'altitude'\n",
    "            alt_variables = [\n",
    "                var_names[ii] for ii, sn in enumerate(standard_names) if sn == 'altitude'\n",
    "            ]\n",
    "            for var_name in alt_variables:\n",
    "                try:\n",
    "                    write_obj[var_name].attrs['axis']\n",
    "                except KeyError:\n",
    "                    write_obj[var_name].attrs['axis'] = 'Z'\n",
    "\n",
    "                try:\n",
    "                    write_obj[var_name].attrs['positive']\n",
    "                except KeyError:\n",
    "                    write_obj[var_name].attrs['positive'] = 'up'\n",
    "\n",
    "            # Check if the Conventions global attribute lists the CF convention\n",
    "            try:\n",
    "                Conventions = write_obj.attrs['Conventions']\n",
    "                Conventions = Conventions.split()\n",
    "                cf_listed = False\n",
    "                for ii in Conventions:\n",
    "                    if ii.startswith('CF-'):\n",
    "                        cf_listed = True\n",
    "                        break\n",
    "                if not cf_listed:\n",
    "                    Conventions.append(cf_convention)\n",
    "                write_obj.attrs['Conventions'] = ' '.join(Conventions)\n",
    "\n",
    "            except KeyError:\n",
    "                write_obj.attrs['Conventions'] = str(cf_convention)\n",
    "\n",
    "            # Reorder global attributes to ensure history is last\n",
    "            try:\n",
    "                global_attrs = write_obj.attrs\n",
    "                history = copy.copy(global_attrs['history'])\n",
    "                del global_attrs['history']\n",
    "                global_attrs['history'] = history\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        write_obj.to_netcdf(encoding=encoding, **kwargs)\n",
    "\n",
    "\n",
    "def check_if_tar_gz_file(filenames):\n",
    "    \"\"\"\n",
    "    Unpacks gunzip and/or TAR file contents and returns Xarray Dataset\n",
    "    ...\n",
    "    Parameters\n",
    "    ----------\n",
    "    filenames : str, pathlib.Path\n",
    "        Filenames to check if gunzip and/or tar files.\n",
    "    Returns\n",
    "    -------\n",
    "    filenames : Paths to extracted files from gunzip or TAR files\n",
    "    \"\"\"\n",
    "\n",
    "    cleanup = False\n",
    "    if isinstance(filenames, (str, PathLike)):\n",
    "        try:\n",
    "            if is_gunzip_file(filenames) or tarfile.is_tarfile(str(filenames)):\n",
    "                tmpdirname = tempfile.mkdtemp()\n",
    "                cleanup = True\n",
    "                if is_gunzip_file(filenames):\n",
    "                    filenames = unpack_gzip(filenames, write_directory=tmpdirname)\n",
    "\n",
    "                if tarfile.is_tarfile(str(filenames)):\n",
    "                    filenames = unpack_tar(filenames, write_directory=tmpdirname, randomize=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return filenames, cleanup\n",
    "\n",
    "\n",
    "def read_mmcr(filenames):\n",
    "    \"\"\"\n",
    "    Reads in ARM MMCR files and splits up the variables into specific\n",
    "    mode variables based on what's in the files.  MMCR files have the modes\n",
    "    interleaved and are not readable using xarray so some modifications are\n",
    "    needed ahead of time.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filenames : str, pathlib.PosixPath or list of str\n",
    "        Name of file(s) to read.\n",
    "    Returns\n",
    "    -------\n",
    "    obj : Object (or None)\n",
    "        ACT dataset (or None if no data file(s) found).\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort the files to make sure they concatenate right\n",
    "    filenames.sort()\n",
    "\n",
    "    # Run through each file and read it in using netCDF4, then\n",
    "    # read it in with xarray\n",
    "    objs = []\n",
    "    for f in filenames:\n",
    "        nc = Dataset(f, \"a\")\n",
    "        # Change heights name to range to read appropriately to xarray\n",
    "        if 'heights' in nc.dimensions:\n",
    "            nc.renameDimension('heights', 'range')\n",
    "        if nc is not None:\n",
    "            obj = xr.open_dataset(xr.backends.NetCDF4DataStore(nc))\n",
    "            objs.append(obj)\n",
    "    # Concatenate objects together\n",
    "    if len(objs) > 1:\n",
    "        obj = xr.concat(objs, dim='time')\n",
    "    else:\n",
    "        obj = objs\n",
    "\n",
    "    # Get mdoes and ranges with time/height modes\n",
    "    modes = obj['mode'].values\n",
    "    mode_vars = []\n",
    "    for v in obj:\n",
    "        if 'range' in obj[v].dims and 'time' in obj[v].dims and len(obj[v].dims) == 2:\n",
    "            mode_vars.append(v)\n",
    "\n",
    "    # For each mode, run extract data variables if available\n",
    "    # saves as individual variables in the file.\n",
    "    for m in modes:\n",
    "        mode_desc = obj['ModeDescription'].values[0, m]\n",
    "        if np.isnan(obj['heights'].values[0, m, :]).all():\n",
    "            continue\n",
    "        mode_desc = str(mode_desc).split('_')[-1][0:-1]\n",
    "        mode_desc = str(mode_desc).split('\\'')[0]\n",
    "        idx = np.where(obj['ModeNum'].values == m)[0]\n",
    "        range_data = obj['heights'].values[0, m, :]\n",
    "        idy = np.where(~np.isnan(range_data))[0]\n",
    "        for v in mode_vars:\n",
    "            new_var_name = v + '_' + mode_desc\n",
    "            time_name = 'time_' + mode_desc\n",
    "            range_name = 'range_' + mode_desc\n",
    "            data = obj[v].values[idx, :]\n",
    "            data = data[:, idy]\n",
    "            attrs = obj[v].attrs\n",
    "            da = xr.DataArray(\n",
    "                data=data,\n",
    "                coords={time_name: obj['time'].values[idx], range_name: range_data[idy]},\n",
    "                dims=[time_name, range_name],\n",
    "                attrs=attrs\n",
    "            )\n",
    "            obj[new_var_name] = da\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_mmcr() missing 1 required positional argument: 'filenames'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m read_mmcr()\n",
      "\u001b[1;31mTypeError\u001b[0m: read_mmcr() missing 1 required positional argument: 'filenames'"
     ]
    }
   ],
   "source": [
    "read_mmcr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07ce037792393486de82b78557509c70dc90bfb6f086cff9f2eca46d4f24b232"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
